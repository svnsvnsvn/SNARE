Secondary dataset links. Want to include the average price/rent per zipcode in the model. 850$ might be normal in the an average zipcode, but would be a big red flag in the most expensive zipcode.
https://www.huduser.gov/portal/datasets/fmr/smallarea/index.html?utm_source=chatgpt.com
https://www.unitedstateszipcodes.org/rankings/zips-in-fl/median_monthly_rent/
http://www.usa.com/rank/florida-state--median-household-income--zip-code-rank.htm
http://www.usa.com/rank/florida-state--house-median-value--zip-code-rank.htm?hl=32202&hlst=FL&yr=9000

Median Rent by City: https://joeshimkus.com/FL-Rent-Costs.aspx
Median Income by City: http://www.usa.com/rank/florida-state--median-household-income--city-rank.htm








Model Improvements:
FUTURE IMPROVEMENTS


Let me explain what I meant by "Use correlation analysis or feature importance scores (e.g., from Isolation Forest) to prioritize the most relevant features and reduce noise" in a clear and straightforward way. This suggestion is about refining the set of features (or variables) you use in a project, like detecting apartment scams, to make your model more effective and efficient.
________________________________________
What Does This Mean?
When you're working with data, you often have many features—like 'Price per Bedroom,' 'Price per Bathroom,' or 'Distance Suspiciousness' in an apartment scam detection project. Not all features are equally useful. Some might overlap in what they tell you (redundant), while others might not matter much or even confuse the model (noise). My suggestion is to use two techniques—correlation analysis and feature importance scores—to figure out which features are worth keeping and which ones you can drop.
________________________________________
1. Correlation Analysis
What It Is
Correlation analysis measures how strongly two features are related to each other. If two features move together (e.g., when one increases, the other does too), they’re highly correlated and might be giving you similar information.
Why It Helps
If two features are very similar, you don’t need both in your model. Including them could make your model slower, harder to understand, or even less accurate because it’s trying to process redundant information. By removing one, you simplify things without losing much.
How to Do It
•	Calculate a correlation coefficient (like Pearson correlation) for each pair of features. This is a number between -1 and 1: 
o	Close to 1: The features increase together (strong positive correlation).
o	Close to -1: One increases as the other decreases (strong negative correlation).
o	Close to 0: Little or no relationship.
•	If two features have a high correlation (say, above 0.8 or below -0.8), consider keeping just one.
Example
Imagine you have 'Price per Bedroom' and 'Price per Total Bathroom.' In many properties, the number of bedrooms and bathrooms are related, so these features might be highly correlated. If they are, you could drop one to avoid redundancy.
________________________________________
2. Feature Importance Scores
What It Is
Feature importance scores tell you how much each feature contributes to your model’s predictions. For example, in an Isolation Forest (a model often used for anomaly detection), you can see which features are most helpful in spotting unusual patterns—like potential scams.
Why It Helps
Not every feature is useful for finding anomalies. Some might be irrelevant or add noise, making it harder for the model to focus on what matters. By identifying the most important features, you can keep those and discard the rest.
How to Do It
•	Train an Isolation Forest model on your data.
•	Look at how the model uses each feature to separate anomalies. While Isolation Forest doesn’t give importance scores directly (like some other models), you can estimate them by: 
o	Checking how often a feature is used to split the data in the model’s trees.
o	Using a technique like permutation importance, which measures how much the model’s performance drops if you shuffle a feature’s values.
•	Rank the features by their scores and keep the top ones (e.g., the top 5) or those above a certain threshold.
Example
Suppose you find that 'Price per Living Area' and 'Distance Suspiciousness' are the most important for detecting scams, while 'Price per Year Built' barely matters. You could focus on the top two and drop the less useful one.
________________________________________
Why Prioritize Features and Reduce Noise?
By using these techniques, you streamline your feature set, which has several benefits:
•	Faster Models: Fewer features mean the model trains and predicts more quickly—great for large datasets or real-time applications like an API.
•	Better Performance: Removing redundant or noisy features helps the model focus on what’s truly important, potentially making it more accurate.
•	Easier to Understand: With fewer features, it’s simpler to explain why the model flags something as suspicious (e.g., “The price per square foot is way off”).
________________________________________
Putting It Together
•	Correlation analysis helps you spot and remove features that overlap too much, like 'Price per Bedroom' and 'Price per Bathroom' if they’re nearly the same.
•	Feature importance scores (e.g., from Isolation Forest) help you prioritize the features that best detect anomalies, like 'Price per Living Area' if it’s a key scam indicator.
Together, these methods cut down noise (irrelevant or confusing data) and highlight the most relevant features, making your model more efficient and effective. For your apartment scam detection project, this could mean faster, more accurate scam predictions with less clutter in the process!

