{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from math import radians, cos, sin, asin, sqrt, log  # Import required math functions\n",
    "import os\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1749, 77)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1749 entries, 0 to 1748\n",
      "Data columns (total 77 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   List Number                           1749 non-null   object \n",
      " 1   Agency Phone                          1736 non-null   object \n",
      " 2   Listing Agent                         1749 non-null   object \n",
      " 3   Co-Listing Agent                      285 non-null    object \n",
      " 4   Property Type                         1749 non-null   object \n",
      " 5   Card Format                           1749 non-null   object \n",
      " 6   Book Section                          1749 non-null   object \n",
      " 7   Property Sub Type                     1749 non-null   object \n",
      " 8   Listing Contract Date                 1749 non-null   object \n",
      " 9   Back on Market Date                   78 non-null     object \n",
      " 10  Status                                1749 non-null   object \n",
      " 11  Status Change Timestamp               1749 non-null   object \n",
      " 12  Original List Price                   1749 non-null   int64  \n",
      " 13  List Price                            1749 non-null   int64  \n",
      " 14  Living Area Source                    1749 non-null   object \n",
      " 15  MLS Area Major                        1749 non-null   int64  \n",
      " 16  Lock Box Number                       22 non-null     object \n",
      " 17  Street Direction Prefix               78 non-null     object \n",
      " 18  Street Direction Suffix               183 non-null    object \n",
      " 19  Non-Representative Compensation Type  1733 non-null   object \n",
      " 20  City                                  1749 non-null   object \n",
      " 21  State                                 1749 non-null   object \n",
      " 22  County                                1749 non-null   object \n",
      " 23  Country                               1704 non-null   object \n",
      " 24  Postal Code                           1748 non-null   float64\n",
      " 25  Stories Total                         1742 non-null   float64\n",
      " 26  Stories                               869 non-null    float64\n",
      " 27  Latitude                              1749 non-null   float64\n",
      " 28  Longitude                             1749 non-null   float64\n",
      " 29  GeoID                                 1749 non-null   float64\n",
      " 30  Living Area                           1749 non-null   float64\n",
      " 31  Year Built                            1749 non-null   int64  \n",
      " 32  Direction Faces                       133 non-null    object \n",
      " 33  Lot Size Dimensions                   25 non-null     object \n",
      " 34  Bedrooms Total                        1749 non-null   int64  \n",
      " 35  Bathrooms Total                       1749 non-null   int64  \n",
      " 36  Bathrooms Full                        1749 non-null   int64  \n",
      " 37  Bathrooms Half                        1748 non-null   float64\n",
      " 38  Parcel Number                         1745 non-null   object \n",
      " 39  Owner Name                            217 non-null    object \n",
      " 40  mod_timestamp                         767 non-null    object \n",
      " 41  Association Fee Frequency             146 non-null    object \n",
      " 42  Occupant Type                         682 non-null    object \n",
      " 43  Listing Agreement                     1733 non-null   object \n",
      " 44  Association Fee 2 Frequency           13 non-null     object \n",
      " 45  Listing Service                       1733 non-null   object \n",
      " 46  Lease Term                            1749 non-null   object \n",
      " 47  Association Fee                       143 non-null    float64\n",
      " 48  Building Area Total                   63 non-null     float64\n",
      " 49  Garage Spaces                         1056 non-null   float64\n",
      " 50  Association Fee 2                     16 non-null     float64\n",
      " 51  List Price/SqFt                       1740 non-null   float64\n",
      " 52  Availability Date                     1092 non-null   object \n",
      " 53  Start Showing Date                    75 non-null     object \n",
      " 54  Association Name                      62 non-null     object \n",
      " 55  Association YN                        1749 non-null   object \n",
      " 56  Carport YN                            1733 non-null   object \n",
      " 57  Property Attached YN                  224 non-null    object \n",
      " 58  Furnished                             854 non-null    object \n",
      " 59  Lot Size Acres                        1283 non-null   float64\n",
      " 60  Contingent Date                       1 non-null      object \n",
      " 61  Senior Community YN                   1734 non-null   object \n",
      " 62  Waterfront YN                         1749 non-null   object \n",
      " 63  Water Frontage Feet                   3 non-null      float64\n",
      " 64  Accessory Dwelling Unit Y/N           222 non-null    object \n",
      " 65  Rental Service                        911 non-null    object \n",
      " 66  Garage YN                             1733 non-null   object \n",
      " 67  Carport Spaces                        87 non-null     float64\n",
      " 68  Non-Representative Compensation       1732 non-null   float64\n",
      " 69  Entry Level                           430 non-null    float64\n",
      " 70  Accessibility Features YN             1732 non-null   object \n",
      " 71  Unit Type                             98 non-null     object \n",
      " 72  Comp Sale YN                          430 non-null    object \n",
      " 73  Price Change Timestamp                361 non-null    object \n",
      " 74  Days on Market                        1749 non-null   int64  \n",
      " 75  Rooms                                 311 non-null    object \n",
      " 76  Features                              1748 non-null   object \n",
      "dtypes: float64(18), int64(8), object(51)\n",
      "memory usage: 1.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['List Number', 'Agency Phone', 'Listing Agent', 'Co-Listing Agent',\n",
       "       'Property Type', 'Card Format', 'Book Section', 'Property Sub Type',\n",
       "       'Listing Contract Date', 'Back on Market Date', 'Status',\n",
       "       'Status Change Timestamp', 'Original List Price', 'List Price',\n",
       "       'Living Area Source', 'MLS Area Major', 'Lock Box Number',\n",
       "       'Street Direction Prefix', 'Street Direction Suffix',\n",
       "       'Non-Representative Compensation Type', 'City', 'State', 'County',\n",
       "       'Country', 'Postal Code', 'Stories Total', 'Stories', 'Latitude',\n",
       "       'Longitude', 'GeoID', 'Living Area', 'Year Built', 'Direction Faces',\n",
       "       'Lot Size Dimensions', 'Bedrooms Total', 'Bathrooms Total',\n",
       "       'Bathrooms Full', 'Bathrooms Half', 'Parcel Number', 'Owner Name',\n",
       "       'mod_timestamp', 'Association Fee Frequency', 'Occupant Type',\n",
       "       'Listing Agreement', 'Association Fee 2 Frequency', 'Listing Service',\n",
       "       'Lease Term', 'Association Fee', 'Building Area Total', 'Garage Spaces',\n",
       "       'Association Fee 2', 'List Price/SqFt', 'Availability Date',\n",
       "       'Start Showing Date', 'Association Name', 'Association YN',\n",
       "       'Carport YN', 'Property Attached YN', 'Furnished', 'Lot Size Acres',\n",
       "       'Contingent Date', 'Senior Community YN', 'Waterfront YN',\n",
       "       'Water Frontage Feet', 'Accessory Dwelling Unit Y/N', 'Rental Service',\n",
       "       'Garage YN', 'Carport Spaces', 'Non-Representative Compensation',\n",
       "       'Entry Level', 'Accessibility Features YN', 'Unit Type', 'Comp Sale YN',\n",
       "       'Price Change Timestamp', 'Days on Market', 'Rooms', 'Features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "raw_housing_data = pd.read_csv(\"data/PrimaryDataset-MLS-RentalProperties.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(raw_housing_data.shape)\n",
    "raw_housing_data.info() # Get summary information about the columns and data types\n",
    "\n",
    "# Display the first few rows of the dataset to inspect it\n",
    "raw_housing_data.head()\n",
    "\n",
    "raw_housing_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Relevant and Numerical Columns\n",
    "In this cell, we focus on selecting specific columns from the dataset for further analysis:\n",
    "\n",
    "<!-- **Relevant Columns:** We define the relevant_columns list, which contains a broader set of columns, including both numerical and categorical data that are deemed important for our analysis. -->\n",
    "\n",
    "**Numerical Columns**: From the relevant_columns, we further narrow down the dataset by selecting only the numerical columns for ease of analysis, such as price, number of bedrooms, bathrooms, area size, etc. These are stored in the numerical_columns list.\n",
    "\n",
    "**Dropping Irrelevant Columns**: Using the numerical_columns, we create a new DataFrame, cleaned_housing_data, which contains only the relevant numerical columns. This step simplifies the dataset and prepares it for further processing, like model training or statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relevant columns to be used in the analysis\n",
    "# relevant_columns = [ 'List Number'\n",
    "#     'List Price', 'Bedrooms Total', 'Bathrooms Total', 'Living Area', \n",
    "#     'MLS Area Major', 'Year Built', \n",
    "#     'Lot Size Acres', 'Days on Market', 'Non-Representative Compensation',\n",
    "#     'Waterfront YN', 'Garage YN', 'Stories Total', 'Stories', 'Bedrooms Total', 'Bathrooms Total',\n",
    "#     'Bathrooms Full', 'Bathrooms Half', 'Garage YN', 'Garage Spaces', 'Original List Price', 'Latitude', 'Longitude', 'Rooms', 'Features'\n",
    "# ]\n",
    "\n",
    "# Select only the numerical columns for analysis\n",
    "numerical_columns = [\n",
    "    'List Number','List Price', 'Bedrooms Total', 'Bathrooms Total', 'Living Area', 'MLS Area Major', 'Year Built', 'Lot Size Acres', 'Days on Market', 'Non-Representative Compensation',\n",
    "    'Stories Total', 'Stories', 'Bathrooms Full', 'Bathrooms Half', 'Garage Spaces', 'Original List Price', 'Latitude', 'Longitude'\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with only the selected numerical columns\n",
    "numerical_housing_data = raw_housing_data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Dataset\n",
    "In this cell, we perform data cleaning operations to ensure that the dataset is suitable for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340, 18)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 340 entries, 24 to 1745\n",
      "Data columns (total 18 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   List Number                      340 non-null    object \n",
      " 1   List Price                       340 non-null    int64  \n",
      " 2   Bedrooms Total                   340 non-null    int64  \n",
      " 3   Bathrooms Total                  340 non-null    int64  \n",
      " 4   Living Area                      340 non-null    float64\n",
      " 5   MLS Area Major                   340 non-null    int64  \n",
      " 6   Year Built                       340 non-null    int64  \n",
      " 7   Lot Size Acres                   340 non-null    float64\n",
      " 8   Days on Market                   340 non-null    int64  \n",
      " 9   Non-Representative Compensation  340 non-null    float64\n",
      " 10  Stories Total                    340 non-null    float64\n",
      " 11  Stories                          340 non-null    float64\n",
      " 12  Bathrooms Full                   340 non-null    int64  \n",
      " 13  Bathrooms Half                   340 non-null    float64\n",
      " 14  Garage Spaces                    340 non-null    float64\n",
      " 15  Original List Price              340 non-null    int64  \n",
      " 16  Latitude                         340 non-null    float64\n",
      " 17  Longitude                        340 non-null    float64\n",
      "dtypes: float64(9), int64(8), object(1)\n",
      "memory usage: 50.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>List Number</th>\n",
       "      <th>List Price</th>\n",
       "      <th>Bedrooms Total</th>\n",
       "      <th>Bathrooms Total</th>\n",
       "      <th>Living Area</th>\n",
       "      <th>MLS Area Major</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>Lot Size Acres</th>\n",
       "      <th>Days on Market</th>\n",
       "      <th>Non-Representative Compensation</th>\n",
       "      <th>Stories Total</th>\n",
       "      <th>Stories</th>\n",
       "      <th>Bathrooms Full</th>\n",
       "      <th>Bathrooms Half</th>\n",
       "      <th>Garage Spaces</th>\n",
       "      <th>Original List Price</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gAAAAABm9wKy8iEF2iPj4yUG9W9aJzlue0ha1rOr4yH2hV...</td>\n",
       "      <td>1695</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>26</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.01</td>\n",
       "      <td>55</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1725</td>\n",
       "      <td>30.284262</td>\n",
       "      <td>-81.454856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>gAAAAABm9wKyZy5yVqVfOab8ncZ2KX9fPYOjfO9KuSTwOh...</td>\n",
       "      <td>2700</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2498.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.46</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2900</td>\n",
       "      <td>30.156717</td>\n",
       "      <td>-81.629576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gAAAAABm9wKyO0FKSTfQtXpJt527HRb4FRUbo6r7zsshdH...</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>135</td>\n",
       "      <td>1980</td>\n",
       "      <td>0.27</td>\n",
       "      <td>53</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2600</td>\n",
       "      <td>30.160414</td>\n",
       "      <td>-81.744717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>gAAAAABm9wKyzMbRTpX1vRA4eWcmaYrxsjpy9YlV1jQyBf...</td>\n",
       "      <td>4425</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2442.0</td>\n",
       "      <td>43</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.10</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4425</td>\n",
       "      <td>30.341041</td>\n",
       "      <td>-81.461648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>gAAAAABm9wKyN72eWI3levdaCpsUo_0YQoDg888YszAwJH...</td>\n",
       "      <td>4500</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1426.0</td>\n",
       "      <td>212</td>\n",
       "      <td>1979</td>\n",
       "      <td>0.14</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4500</td>\n",
       "      <td>30.266153</td>\n",
       "      <td>-81.398813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          List Number  List Price  \\\n",
       "24  gAAAAABm9wKy8iEF2iPj4yUG9W9aJzlue0ha1rOr4yH2hV...        1695   \n",
       "26  gAAAAABm9wKyZy5yVqVfOab8ncZ2KX9fPYOjfO9KuSTwOh...        2700   \n",
       "37  gAAAAABm9wKyO0FKSTfQtXpJt527HRb4FRUbo6r7zsshdH...        2500   \n",
       "39  gAAAAABm9wKyzMbRTpX1vRA4eWcmaYrxsjpy9YlV1jQyBf...        4425   \n",
       "40  gAAAAABm9wKyN72eWI3levdaCpsUo_0YQoDg888YszAwJH...        4500   \n",
       "\n",
       "    Bedrooms Total  Bathrooms Total  Living Area  MLS Area Major  Year Built  \\\n",
       "24               2                2       1230.0              26        2008   \n",
       "26               5                2       2498.0              14        1996   \n",
       "37               5                2       1990.0             135        1980   \n",
       "39               3                3       2442.0              43        2012   \n",
       "40               3                2       1426.0             212        1979   \n",
       "\n",
       "    Lot Size Acres  Days on Market  Non-Representative Compensation  \\\n",
       "24            0.01              55                             10.0   \n",
       "26            0.46              43                              0.0   \n",
       "37            0.27              53                            100.0   \n",
       "39            0.10              23                              1.0   \n",
       "40            0.14              21                              1.0   \n",
       "\n",
       "    Stories Total  Stories  Bathrooms Full  Bathrooms Half  Garage Spaces  \\\n",
       "24            2.0      2.0               2             0.0            1.0   \n",
       "26            2.0      2.0               2             0.0            2.0   \n",
       "37            1.0      1.0               2             0.0            2.0   \n",
       "39            2.0      2.0               2             1.0            2.0   \n",
       "40            2.0      2.0               2             0.0            1.0   \n",
       "\n",
       "    Original List Price   Latitude  Longitude  \n",
       "24                 1725  30.284262 -81.454856  \n",
       "26                 2900  30.156717 -81.629576  \n",
       "37                 2600  30.160414 -81.744717  \n",
       "39                 4425  30.341041 -81.461648  \n",
       "40                 4500  30.266153 -81.398813  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with any missing values\n",
    "numerical_housing_data = numerical_housing_data.dropna()\n",
    "\n",
    "# Check for duplicate columns and remove them\n",
    "numerical_housing_data = numerical_housing_data.loc[:, ~numerical_housing_data.columns.duplicated()]\n",
    "\n",
    "# Output the dimensions of the cleaned dataset and preview its structure\n",
    "print(numerical_housing_data.shape)\n",
    "numerical_housing_data.info()\n",
    "numerical_housing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Cleaned Dataset\n",
    "In this cell, we aim to save the cleaned data to a CSV file for future use, while ensuring that we don't overwrite an existing file unintentionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as data/numerical_housing_data.csv\n"
     ]
    }
   ],
   "source": [
    "# File path to save the CSV\n",
    "numerical_file_path = 'data/numerical_housing_data.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(numerical_file_path):\n",
    "    numerical_housing_data.to_csv(numerical_file_path, index=False)\n",
    "    print(f\"File saved as {numerical_file_path}\")\n",
    "else:\n",
    "    print(f\"File {numerical_file_path} already exists. No action taken.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Engineered Features\n",
    "In this cell, we create new price ratio features using the data from the numerical_housing_data DataFrame. Instead of relying solely on the list price, these ratios offer more accurate and meaningful measures of the propertyâ€™s value by adjusting for specific characteristics like the number of bedrooms, bathrooms, and square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/t5/ksv476hn4cq52bmfy8685cnh0000gn/T/ipykernel_40032/674319615.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Add engineered price metrics to the main DataFrame\n",
    "numerical_housing_data['Price per Bedroom'] = numerical_housing_data['List Price'] / numerical_housing_data['Bedrooms Total']\n",
    "numerical_housing_data['Price per Full Bathroom'] = numerical_housing_data['List Price'] / numerical_housing_data['Bathrooms Full']\n",
    "numerical_housing_data['Price per Total Bathroom'] = numerical_housing_data['List Price'] / numerical_housing_data['Bathrooms Total']\n",
    "numerical_housing_data['Price per Story'] = numerical_housing_data['List Price'] / numerical_housing_data['Stories']\n",
    "numerical_housing_data['Price per Garage Space'] = numerical_housing_data['List Price'] / numerical_housing_data['Garage Spaces']\n",
    "numerical_housing_data['Price per Living Area'] = numerical_housing_data['List Price'] / numerical_housing_data['Living Area']\n",
    "numerical_housing_data['Price per Lot Size Acre'] = numerical_housing_data['List Price'] / numerical_housing_data['Lot Size Acres']\n",
    "numerical_housing_data['Price per Year Built'] = numerical_housing_data['List Price'] / numerical_housing_data['Year Built']\n",
    "\n",
    "# Handle NaN and infinity values for consistency in the main DataFrame\n",
    "columns_to_fix = ['Price per Bedroom', 'Price per Full Bathroom', 'Price per Total Bathroom', \n",
    "                  'Price per Story', 'Price per Garage Space', 'Price per Living Area', \n",
    "                  'Price per Lot Size Acre', 'Price per Year Built']\n",
    "\n",
    "# Fix NaN and infinite values\n",
    "for col in columns_to_fix:\n",
    "    numerical_housing_data[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    numerical_housing_data[col].fillna(numerical_housing_data[col].median(), inplace=True)\n",
    "\n",
    "# Now, all price metrics are stored within numerical_housing_data, and we can proceed with other features or flags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of detecting fraudulent or nefarious listings, the `Under_30_Days_Flag` provides important insights into the listing behavior:\n",
    "1. Rapid Turnover Can Indicate Fraud:\n",
    "    Listings that are on the market for 30 days or less and then disappear could be suspicious. Scammers may list fake properties, get quick responses, and remove the listing once they've attracted victims. A high turnover rate may suggest that the listing is either underpriced to lure in victims or that the listing is fake and meant to disappear quickly.\n",
    "2. Uncommon Market Behavior:\n",
    "    In most real estate markets, properties tend to stay listed for more than 30 days unless they are highly desirable or misrepresented. Listings with a flag of 1 (30 days or less) can be used as a potential red flag for deeper analysis, especially when combined with other indicators like price anomalies or inconsistent agent information.\n",
    "\n",
    "3. Behavioral Clustering:\n",
    "    By using the Under_30_Days_Flag, we can cluster or categorize properties based on their time on the market. If a cluster of properties flagged as \"Under 30 Days\" shares other suspicious characteristics (e.g., odd pricing, proximity to multiple points of interest), it strengthens the likelihood that the listings are fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Days on Market  Under_30_Days_Flag\n",
      "24              55                   0\n",
      "26              43                   0\n",
      "37              53                   0\n",
      "39              23                   1\n",
      "40              21                   1\n"
     ]
    }
   ],
   "source": [
    "# Add the Under_30_Days_Flag (1 = 30 days or less, 0 = more than 30 days)\n",
    "numerical_housing_data['Under_30_Days_Flag'] = numerical_housing_data['Days on Market'].apply(lambda x: 1 if x <= 30 else 0)\n",
    "\n",
    "# Verify the first few rows of the updated dataset\n",
    "print(numerical_housing_data[['Days on Market', 'Under_30_Days_Flag']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suspiciousness Score Based on Proximity to Important Locations\n",
    "In this cell, we calculate a suspicion score for each listing based on its proximity to important locations such as universities and military bases. Listings close to high-traffic or important areas might be more likely to exhibit suspicious behavior, such as underpricing or quick turnover, which could indicate fraudulent activity.\n",
    "\n",
    "1. Important Locations:\n",
    "A list of important locations (e.g., universities, military bases) is used to compute suspiciousness scores. Each location is assigned a weight based on its importance, such as the size of the population it serves.\n",
    "2. Suspiciousness Calculation:\n",
    "We use the Haversine formula to calculate the distance between the listing and important locations. A suspiciousness score is calculated using a weighted logarithmic distance function, where listings closer to high-weight locations have higher suspiciousness scores.\n",
    "3. Filtering and Visualization:\n",
    "Listings without latitude or longitude are filtered out. We then visualize the suspiciousness scores on a heatmap using Folium, with the intensity of the color indicating higher levels of suspicion.\n",
    "4. Heatmap:\n",
    "The heatmap is centered around Duval County, and it is saved as an HTML file for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of important locations with weights\n",
    "important_coordinates = [\n",
    "    {\"name\": \"University of North Florida\", \"latitude\": 30.2715, \"longitude\": -81.5094, \"weight\": 0.7727},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Flagler College\", \"latitude\": 29.8947, \"longitude\": -81.3145, \"weight\": 0.1182},  # St. Augustine, St. Johns County\n",
    "    {\"name\": \"St. Johns River State College\", \"latitude\": 29.6486, \"longitude\": -81.6417, \"weight\": 0.2955},  # Palatka, Putnam County\n",
    "    {\"name\": \"Edward Waters University\", \"latitude\": 30.3422, \"longitude\": -81.6794, \"weight\": 0.0455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Concorde Career Institute\", \"latitude\": 30.3374, \"longitude\": -81.5546, \"weight\": 0.0227},  # Jacksonville, Duval County\n",
    "    {\"name\": \"First Coast Technical College\", \"latitude\": 29.8922, \"longitude\": -81.3305, \"weight\": 0.0182},  # St. Augustine, St. Johns County\n",
    "    {\"name\": \"Jacksonville University\", \"latitude\": 30.3532, \"longitude\": -81.6068, \"weight\": 0.2045},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Jones Technical Institute\", \"latitude\": 30.2449, \"longitude\": -81.5322, \"weight\": 0.0182},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Tulsa Welding School\", \"latitude\": 30.3385, \"longitude\": -81.5637, \"weight\": 0.0136},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Chamberlain University-Florida\", \"latitude\": 30.2598, \"longitude\": -81.5904, \"weight\": 0.0409},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Fortis College-Orange Park\", \"latitude\": 30.1785, \"longitude\": -81.7079, \"weight\": 0.0318},  # Orange Park, Clay County\n",
    "    {\"name\": \"Florida State College at Jacksonville\", \"latitude\": 30.3322, \"longitude\": -81.6557, \"weight\": 1.0000},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Trinity Baptist College\", \"latitude\": 30.2395, \"longitude\": -81.7802, \"weight\": 0.0227},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Keiser University\", \"latitude\": 30.3326, \"longitude\": -81.6562, \"weight\": 0.0455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Heritage Institute\", \"latitude\": 30.2033, \"longitude\": -81.5837, \"weight\": 0.0182},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Embry-Riddle Aeronautical University\", \"latitude\": 29.1880, \"longitude\": -81.0479, \"weight\": 0.4091},  # Daytona Beach, Volusia County\n",
    "    {\"name\": \"Naval Air Station Jacksonville\", \"latitude\": 30.2358, \"longitude\": -81.6800, \"weight\": 0.9545},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Naval Station Mayport\", \"latitude\": 30.3915, \"longitude\": -81.4245, \"weight\": 0.5455},  # Jacksonville, Duval County\n",
    "    {\"name\": \"Camp Blanding Joint Training Center\", \"latitude\": 29.9693, \"longitude\": -81.9840, \"weight\": 0.6818},  # Clay County\n",
    "    {\"name\": \"Marine Corps Blount Island Command\", \"latitude\": 30.4111, \"longitude\": -81.5059, \"weight\": 0.1364},  # Jacksonville, Duval County\n",
    "]\n",
    "\n",
    "# Haversine formula to calculate the distance between two points\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6371 * c  # Radius of Earth in kilometers\n",
    "    return km\n",
    "\n",
    "# Function to calculate suspicion score based on proximity to important locations\n",
    "def calculate_weighted_suspiciousness(row, important_locations, max_distance=50, scaling_factor=1):\n",
    "    listing_lat = row['Latitude']\n",
    "    listing_lon = row['Longitude']\n",
    "    \n",
    "    total_suspiciousness = 0\n",
    "    baseline_suspiciousness = 0.05  # To account for listings far from important locations\n",
    "    \n",
    "    for location in important_locations:\n",
    "        dist = haversine(listing_lat, listing_lon, location['latitude'], location['longitude'])\n",
    "        \n",
    "        # Only compute suspiciousness for distances within the max limit\n",
    "        if dist <= max_distance:\n",
    "            location_suspiciousness = location['weight'] * (1 / (log(dist + 1) + scaling_factor))\n",
    "            total_suspiciousness += location_suspiciousness\n",
    "    \n",
    "    # Add baseline score if total suspiciousness is very low\n",
    "    total_suspiciousness = max(baseline_suspiciousness, total_suspiciousness)\n",
    "    \n",
    "    return total_suspiciousness\n",
    "\n",
    "# Apply the suspicion calculation to each listing in the dataset\n",
    "numerical_housing_data['Distance Suspiciousness'] = numerical_housing_data.apply(lambda row: calculate_weighted_suspiciousness(row, important_coordinates), axis=1)\n",
    "\n",
    "# Create a map centered around Duval County\n",
    "m = folium.Map(location=[30.3322, -81.6557], zoom_start=10)\n",
    "\n",
    "# Create a list of coordinates and weights (suspicion scores) for the heatmap\n",
    "heatmap_data = [[row['Latitude'], row['Longitude'], row['Distance Suspiciousness']] for index, row in numerical_housing_data.iterrows()]\n",
    "\n",
    "# Add the heatmap layer with suspicion scores\n",
    "HeatMap(heatmap_data, max_value=1, radius=15, blur=10).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('suspicion_score_heatmap.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Distance Suspiciousness'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/Python/Personal/SLATE/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Distance Suspiciousness'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mnumerical_housing_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDistance Suspiciousness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Add titles and labels\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution of Distance Suspiciousness\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/Python/Personal/SLATE/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Projects/Python/Personal/SLATE/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Distance Suspiciousness'"
     ]
    }
   ],
   "source": [
    "plt.hist(numerical_housing_data['Distance Suspiciousness'], bins=30, edgecolor='black')\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Distance Suspiciousness')\n",
    "plt.xlabel('Suspiciousness Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Suspicious Phone Numbers Based on Multiple Agent Associations\n",
    "In this cell, we aim to detect suspicious phone numbers by identifying numbers that are associated with more than one unique listing agent. These suspicious phone numbers could indicate potential fraud, such as a single person or entity listing properties under multiple names to create the illusion of different agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['List Number', 'List Price', 'Bedrooms Total', 'Bathrooms Total',\n",
      "       'Living Area', 'MLS Area Major', 'Year Built', 'Lot Size Acres',\n",
      "       'Days on Market', 'Non-Representative Compensation', 'Stories Total',\n",
      "       'Stories', 'Bathrooms Full', 'Bathrooms Half', 'Garage Spaces',\n",
      "       'Original List Price', 'Latitude', 'Longitude', 'Price per Bedroom',\n",
      "       'Price per Full Bathroom', 'Price per Total Bathroom',\n",
      "       'Price per Story', 'Price per Garage Space', 'Price per Living Area',\n",
      "       'Price per Lot Size Acre', 'Price per Year Built', 'Under_30_Days_Flag',\n",
      "       'Distance Suspiciousness', 'Listing Agent_x', 'Agency Phone_x',\n",
      "       'is_phone_suspicious', 'Listing Agent_y', 'Agency Phone_y',\n",
      "       'Listing Agent', 'Agency Phone'],\n",
      "      dtype='object')\n",
      "Index(['List Number', 'Agency Phone', 'Listing Agent', 'Co-Listing Agent',\n",
      "       'Property Type', 'Card Format', 'Book Section', 'Property Sub Type',\n",
      "       'Listing Contract Date', 'Back on Market Date', 'Status',\n",
      "       'Status Change Timestamp', 'Original List Price', 'List Price',\n",
      "       'Living Area Source', 'MLS Area Major', 'Lock Box Number',\n",
      "       'Street Direction Prefix', 'Street Direction Suffix',\n",
      "       'Non-Representative Compensation Type', 'City', 'State', 'County',\n",
      "       'Country', 'Postal Code', 'Stories Total', 'Stories', 'Latitude',\n",
      "       'Longitude', 'GeoID', 'Living Area', 'Year Built', 'Direction Faces',\n",
      "       'Lot Size Dimensions', 'Bedrooms Total', 'Bathrooms Total',\n",
      "       'Bathrooms Full', 'Bathrooms Half', 'Parcel Number', 'Owner Name',\n",
      "       'mod_timestamp', 'Association Fee Frequency', 'Occupant Type',\n",
      "       'Listing Agreement', 'Association Fee 2 Frequency', 'Listing Service',\n",
      "       'Lease Term', 'Association Fee', 'Building Area Total', 'Garage Spaces',\n",
      "       'Association Fee 2', 'List Price/SqFt', 'Availability Date',\n",
      "       'Start Showing Date', 'Association Name', 'Association YN',\n",
      "       'Carport YN', 'Property Attached YN', 'Furnished', 'Lot Size Acres',\n",
      "       'Contingent Date', 'Senior Community YN', 'Waterfront YN',\n",
      "       'Water Frontage Feet', 'Accessory Dwelling Unit Y/N', 'Rental Service',\n",
      "       'Garage YN', 'Carport Spaces', 'Non-Representative Compensation',\n",
      "       'Entry Level', 'Accessibility Features YN', 'Unit Type', 'Comp Sale YN',\n",
      "       'Price Change Timestamp', 'Days on Market', 'Rooms', 'Features'],\n",
      "      dtype='object')\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(numerical_housing_data.columns)\n",
    "print(raw_housing_data.columns)\n",
    "print(numerical_housing_data['List Number'].duplicated().sum())\n",
    "print(raw_housing_data['List Number'].duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "print(numerical_housing_data['List Number'].dtype)\n",
    "print(raw_housing_data['List Number'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           List Number  List Price  \\\n",
      "0    gAAAAABm9wKy8iEF2iPj4yUG9W9aJzlue0ha1rOr4yH2hV...        1695   \n",
      "1    gAAAAABm9wKyZy5yVqVfOab8ncZ2KX9fPYOjfO9KuSTwOh...        2700   \n",
      "2    gAAAAABm9wKyO0FKSTfQtXpJt527HRb4FRUbo6r7zsshdH...        2500   \n",
      "3    gAAAAABm9wKyzMbRTpX1vRA4eWcmaYrxsjpy9YlV1jQyBf...        4425   \n",
      "4    gAAAAABm9wKyN72eWI3levdaCpsUo_0YQoDg888YszAwJH...        4500   \n",
      "..                                                 ...         ...   \n",
      "327  gAAAAABm9wKy4l8o4r_XnMbd9SBNRZZoxejrP1PVND5ugM...        4050   \n",
      "331  gAAAAABm9wKyZ7oyYD1xjxFZxxKXEh4OlVsFjgug-4QJqk...        3200   \n",
      "334  gAAAAABm9wKy3EKc9CTpbCMDFCnW0hOmDro6Bu-gIDu_4X...        3400   \n",
      "338  gAAAAABm9wKyKdxHsPqZYBHf9lHdjPWGImYwIRwLd3NEmP...        1620   \n",
      "339  gAAAAABm9wKy_Kh_pxh7FOqfUuzhjUAQuSn4K3cfqHvILr...        3550   \n",
      "\n",
      "     Bedrooms Total  Bathrooms Total  Living Area  MLS Area Major  Year Built  \\\n",
      "0                 2                2       1230.0              26        2008   \n",
      "1                 5                2       2498.0              14        1996   \n",
      "2                 5                2       1990.0             135        1980   \n",
      "3                 3                3       2442.0              43        2012   \n",
      "4                 3                2       1426.0             212        1979   \n",
      "..              ...              ...          ...             ...         ...   \n",
      "327               3                3       2326.0             263        2006   \n",
      "331               2                2       1969.0             272        2024   \n",
      "334               3                3       2298.0             272        2013   \n",
      "338               2                3       1346.0             304        2006   \n",
      "339               4                2       1779.0             272        2022   \n",
      "\n",
      "     Lot Size Acres  Days on Market  Non-Representative Compensation  ...  \\\n",
      "0              0.01              55                             10.0  ...   \n",
      "1              0.46              43                              0.0  ...   \n",
      "2              0.27              53                            100.0  ...   \n",
      "3              0.10              23                              1.0  ...   \n",
      "4              0.14              21                              1.0  ...   \n",
      "..              ...             ...                              ...  ...   \n",
      "327            0.00              54                              0.0  ...   \n",
      "331            0.16              12                             10.0  ...   \n",
      "334            0.12              62                             10.0  ...   \n",
      "338            0.05              37                             20.0  ...   \n",
      "339            0.16              37                             10.0  ...   \n",
      "\n",
      "     Price per Year Built  Under_30_Days_Flag  Distance Suspiciousness  \\\n",
      "0                0.844124                   0                 1.083409   \n",
      "1                1.352705                   0                 1.170395   \n",
      "2                1.262626                   0                 1.120748   \n",
      "3                2.199304                   1                 1.054096   \n",
      "4                2.273876                   1                 0.997919   \n",
      "..                    ...                 ...                      ...   \n",
      "327              2.018943                   0                 0.924117   \n",
      "331              1.581028                   1                 0.881558   \n",
      "334              1.689021                   0                 0.926952   \n",
      "338              0.807577                   0                 1.102019   \n",
      "339              1.755687                   0                 0.883774   \n",
      "\n",
      "              Listing Agent_x          Agency Phone_x  is_phone_suspicious  \\\n",
      "0     Listing_Agent_Index_997  Agency_Phone_Index_677                    1   \n",
      "1     Listing_Agent_Index_999  Agency_Phone_Index_656                    1   \n",
      "2    Listing_Agent_Index_1009  Agency_Phone_Index_688                    1   \n",
      "3    Listing_Agent_Index_1010  Agency_Phone_Index_689                    1   \n",
      "4    Listing_Agent_Index_1011  Agency_Phone_Index_687                    1   \n",
      "..                        ...                     ...                  ...   \n",
      "327  Listing_Agent_Index_1616  Agency_Phone_Index_736                    1   \n",
      "331  Listing_Agent_Index_1625  Agency_Phone_Index_722                    1   \n",
      "334  Listing_Agent_Index_1320  Agency_Phone_Index_656                    1   \n",
      "338  Listing_Agent_Index_1031  Agency_Phone_Index_702                    1   \n",
      "339  Listing_Agent_Index_1448  Agency_Phone_Index_727                    1   \n",
      "\n",
      "              Listing Agent_y          Agency Phone_y  \\\n",
      "0     Listing_Agent_Index_997  Agency_Phone_Index_677   \n",
      "1     Listing_Agent_Index_999  Agency_Phone_Index_656   \n",
      "2    Listing_Agent_Index_1009  Agency_Phone_Index_688   \n",
      "3    Listing_Agent_Index_1010  Agency_Phone_Index_689   \n",
      "4    Listing_Agent_Index_1011  Agency_Phone_Index_687   \n",
      "..                        ...                     ...   \n",
      "327  Listing_Agent_Index_1616  Agency_Phone_Index_736   \n",
      "331  Listing_Agent_Index_1625  Agency_Phone_Index_722   \n",
      "334  Listing_Agent_Index_1320  Agency_Phone_Index_656   \n",
      "338  Listing_Agent_Index_1031  Agency_Phone_Index_702   \n",
      "339  Listing_Agent_Index_1448  Agency_Phone_Index_727   \n",
      "\n",
      "                Listing Agent            Agency Phone  \n",
      "0     Listing_Agent_Index_997  Agency_Phone_Index_677  \n",
      "1     Listing_Agent_Index_999  Agency_Phone_Index_656  \n",
      "2    Listing_Agent_Index_1009  Agency_Phone_Index_688  \n",
      "3    Listing_Agent_Index_1010  Agency_Phone_Index_689  \n",
      "4    Listing_Agent_Index_1011  Agency_Phone_Index_687  \n",
      "..                        ...                     ...  \n",
      "327  Listing_Agent_Index_1616  Agency_Phone_Index_736  \n",
      "331  Listing_Agent_Index_1625  Agency_Phone_Index_722  \n",
      "334  Listing_Agent_Index_1320  Agency_Phone_Index_656  \n",
      "338  Listing_Agent_Index_1031  Agency_Phone_Index_702  \n",
      "339  Listing_Agent_Index_1448  Agency_Phone_Index_727  \n",
      "\n",
      "[124 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Merge based on 'List Number' as the unique identifier\n",
    "# Drop 'Listing Agent' and 'Agency Phone' in numerical_housing_data if they exist\n",
    "numerical_housing_data = numerical_housing_data.drop(columns=['Listing Agent', 'Agency Phone'], errors='ignore')\n",
    "\n",
    "# Step 1: Merge based on 'List Number' as the unique identifier\n",
    "numerical_housing_data = numerical_housing_data.merge(\n",
    "    raw_housing_data[['List Number', 'Listing Agent', 'Agency Phone']], \n",
    "    on='List Number', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "# Step 2: Group by 'Agency Phone' and aggregate the unique 'Listing Agent' values\n",
    "duplicates = numerical_housing_data.groupby('Agency Phone')['Listing Agent'].nunique()\n",
    "\n",
    "# Step 3: Filter for phone numbers that are associated with more than one unique agent name\n",
    "suspicious_numbers = duplicates[duplicates > 1]\n",
    "\n",
    "# Step 4: Flag listings that contain suspicious phone numbers\n",
    "def flag_suspicious_phone(row):\n",
    "    if row['Agency Phone'] in suspicious_numbers.index:\n",
    "        return 1  # Flag as suspicious\n",
    "    return 0  # Not suspicious\n",
    "\n",
    "# Step 5: Apply the function to the raw_housing_data DataFrame\n",
    "numerical_housing_data['is_phone_suspicious'] = numerical_housing_data.apply(flag_suspicious_phone, axis=1)\n",
    "\n",
    "# Step 6: Display flagged listings\n",
    "suspicious_phone_listings = numerical_housing_data[numerical_housing_data['is_phone_suspicious'] == 1]\n",
    "print(suspicious_phone_listings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Principal Component Analysis (PCA) and Anomaly Detection\n",
    "\n",
    "In this section, we apply PCA for dimensionality reduction and anomaly detection on housing data.\n",
    "We will retain 95% of variance and also explore anomaly detection using DBSCAN and Isolation Forest.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Fit PCA and Transform the Data\n",
    "Here, we fit PCA to the scaled dataset and retain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA (retaining 95% of variance)\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(cleaned_housing_data)\n",
    "\n",
    "print(x_scaled.shape)\n",
    "pca_model = PCA(n_components=0.95)\n",
    "x_pca = pca_model.fit_transform(x_scaled)  # 'x_scaled' is the scaled input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Create a DataFrame for the Principal Components\n",
    "\n",
    "We create a DataFrame that contains the principal components for each data point.\n",
    "Each component captures the variance in the original features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create column names for PCA components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "\n",
    "# Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "pca_df.head()\n",
    "\n",
    "model1 = IsolationForest(n_estimators=100, max_samples='auto', contamination='auto', random_state=12)\n",
    "\n",
    "model1.fit(x_pca)\n",
    "\n",
    "predict = model1.predict(x_pca)\n",
    "anomalies = x_scaled[predict == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA as before\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Step 2: Create a DataFrame for the principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])] \n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 3: Get the contributions of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "print(loadings.shape)\n",
    "print(len(numerical_columns))\n",
    "contributions_df = pd.DataFrame(loadings, index=numerical_columns, columns=pca_columns)\n",
    "\n",
    "\n",
    "# Step 4: Get the top contributing feature for each principal component (for labeling purposes)\n",
    "top_features_per_pc = contributions_df.abs().idxmax()\n",
    "\n",
    "# Step 5: Create a box plot and label each principal component by its strongest contributing original feature\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(data=pca_df)\n",
    "\n",
    "# Rotate the text diagonally and align it so it reads from top to bottom\n",
    "plt.xticks(range(len(top_features_per_pc)), top_features_per_pc, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Title\n",
    "plt.title('Boxplot for Each Principal Component - Labeled by Top Contributing Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA as before\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Step 2: Create a DataFrame for the principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 3: Get the contributions of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "contributions_df = pd.DataFrame(loadings, index=numerical_columns, columns=pca_columns)\n",
    "\n",
    "# Step 4: Get the top contributing feature for each principal component (for labeling purposes)\n",
    "top_features_per_pc = contributions_df.abs().idxmax()\n",
    "\n",
    "# Step 5: Add anomaly labels to the PCA DataFrame\n",
    "pca_df['anomaly'] = predict  # Anomalies (-1) and normal points (1)\n",
    "\n",
    "# Step 6: Parallel Coordinates Plot with PCA data and anomaly labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "parallel_coordinates(pca_df, 'anomaly', color=['blue', 'red'])\n",
    "\n",
    "# Step 7: Adjust the x-axis labels (principal components) with top contributing original feature names\n",
    "plt.xticks(range(len(top_features_per_pc)), top_features_per_pc, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Title\n",
    "plt.title('Parallel Coordinates Plot for PCA-Transformed Housing Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit the PCA as before\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% of variance\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "\n",
    "# Step 2: Create a DataFrame for the principal components\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 3: Get the contributions of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "contributions_df = pd.DataFrame(loadings, index=numerical_columns, columns=pca_columns)\n",
    "\n",
    "# Step 4: Get the top contributing feature for each principal component (for labeling purposes)\n",
    "top_features_per_pc = contributions_df.abs().idxmax()\n",
    "\n",
    "# Step 5: Filter the PCA data to show only anomalies (-1)\n",
    "anomalous_data = pca_df[predict == -1]\n",
    "\n",
    "# Step 6: Plot a heatmap of the anomalous entries (use PCA-transformed data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(anomalous_data, cmap='coolwarm', annot=False, linewidths=0.5)\n",
    "\n",
    "# Step 7: Adjust x-axis labels to reflect original features (diagonal or vertical)\n",
    "plt.xticks(range(len(top_features_per_pc)), top_features_per_pc, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Title\n",
    "plt.title('Heatmap of Anomalous Entries Across Principal Components (PCA-Transformed Data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit DBSCAN on the PCA-transformed data\n",
    "dbscan_model = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps and min_samples based on your data\n",
    "dbscan_labels = dbscan_model.fit_predict(x_pca)\n",
    "\n",
    "# Step 2: Add DBSCAN cluster labels to the PCA DataFrame\n",
    "pca_df['dbscan_cluster'] = dbscan_labels\n",
    "\n",
    "# Step 3: Visualize the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=pca_df['dbscan_cluster'], palette='viridis')\n",
    "plt.title('DBSCAN Clustering on PCA-Transformed Data')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Cross-examine DBSCAN clusters with Isolation Forest results\n",
    "pca_df['anomaly_iforest'] = predict  # Add Isolation Forest anomaly labels (-1 for anomalies, 1 for normal)\n",
    "\n",
    "# Compare clusters and anomalies\n",
    "cross_examined = pca_df[(pca_df['dbscan_cluster'] != -1) & (pca_df['anomaly_iforest'] == -1)]\n",
    "\n",
    "print(f\"Number of points flagged by both DBSCAN and Isolation Forest: {len(cross_examined)}\")\n",
    "cross_examined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_words = [\n",
    "    'urgent', 'alert', 'wire transfer', 'guaranteed', 'free',\n",
    "    'cash only', 'as seen on', 'limited time', 'donâ€™t miss out',\n",
    "    'risk-free', 'act now', 'exclusive', 'once in a lifetime',\n",
    "   'contact now', 'no credit check', 'easy approval','Foreclosure',\n",
    "     \"no deposit required\", \"move-in specials\", \"free month rent\", \"lease takeover\", \"rent-to-own\", \n",
    "    \"pre-approval needed\", \"urgent rental\", \"hurry, limited time offer\", \"cash only, no checks\", \n",
    "    \"first month free\", \"no background check\", \"instant approval\", \"no credit history needed\", \n",
    "    \"temporary housing\", \"assume the lease\", \"short-term rental\", \"virtual tour only\", \n",
    "    \"sublease opportunity\", \"guaranteed approval\", \"utilities included\", \"all bills paid\", \n",
    "   \"no application fee\", \"get approved today\", \"house sitting\", \"unbelievably low rent\", \n",
    "    \"no lease required\", \"instant income\", \"non-refundable deposit\", \"limited properties available\", \n",
    "    \"you won't believe the price\", \"exclusive listings\", \"flexible terms\", \"unforeseen circumstances\", \n",
    "    \"background check waived\", \"contact immediately\", \"first come, first served\", \"urgent need to rent\", \n",
    "    \"newly renovated\", \"donâ€™t get left out\", \"act fast before itâ€™s gone\", \"scam-free guarantee\", \n",
    "    \"friendly landlord\", \"best value rental\", \"quick approval process\", \"no hassle, no fees\", \n",
    "    \"all-inclusive rental\", \"hidden gem\", \"affordable living\", \"ideal for students\", \n",
    "    \"rent today, move in tomorrow\"\n",
    "]\n",
    "\n",
    "def flag_suspicious_listings(row):\n",
    "    # Join relevant columns into one text\n",
    "    text = f\"{row['Features']}\".lower()  # Combine and convert to lowercase\n",
    "    # Check for suspicious words\n",
    "    for word in suspicious_words:\n",
    "        if word in text:\n",
    "            return 1  # Flag as suspicious\n",
    "    return 0  # Not suspicious\n",
    "\n",
    "# Assuming Features is the column to check\n",
    "raw_housing_data['is_suspicious'] = raw_housing_data.apply(flag_suspicious_listings, axis=1)\n",
    "\n",
    "# Display flagged listings\n",
    "suspicious_listings = raw_housing_data[raw_housing_data['is_suspicious'] == 1]\n",
    "print(suspicious_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Start with the base cleaned DataFrame\n",
    "final_df = cleaned_housing_data.copy()\n",
    "\n",
    "# Step 2: Add the new features you created\n",
    "# Assuming you have DataFrames or Series for these additional features, like df_price_metrics, proximity_scores, etc.\n",
    "\n",
    "# Example: Add price metrics\n",
    "final_df = pd.concat([final_df, df_price_metrics], axis=1)\n",
    "\n",
    "# Example: Add scammy words flag (assuming 'is_suspicious' is a column in the raw_housing_data)\n",
    "final_df['suspicious_diction'] = raw_housing_data['is_suspicious']\n",
    "\n",
    "final_df['phone_is_suspicious'] =  raw_housing_data['is_phone_suspicious']\n",
    "# Step 3: Clean the final DataFrame\n",
    "# Handle NaN values, infinity, and other inconsistencies\n",
    "final_df.fillna(final_df.median(), inplace=True)  # Fill NaNs with the median of each column\n",
    "\n",
    "\n",
    "# Step 4: Check the final DataFrame\n",
    "final_df.info()  # To see if everything looks good\n",
    "final_df.head()  # To preview the first few rows\n",
    "\n",
    "\n",
    "file_path = 'data/final_training_set.csv'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_path):\n",
    "    final_df.to_csv(file_path, index=False)\n",
    "    print(f\"File saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns\n",
    "\n",
    "columns_to_drop = ['List Price', 'Bedrooms Total', 'Bathrooms Total', 'Living Area',\n",
    "                   'MLS Area Major', 'Year Built', 'Lot Size Acres', 'Days on Market',\n",
    "                   'Non-Representative Compensation', 'Stories Total', 'Stories',\n",
    "                   'Bathrooms Full', 'Bathrooms Half', 'Garage Spaces',\n",
    "                   'Original List Price', 'Latitude', 'Longitude']\n",
    "\n",
    "# Drop the columns\n",
    "final_df = final_df.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "'''Over_30_Days_Flag',\n",
    "       'Distance Suspiciousness', 'Price per Bedroom',\n",
    "       'Price per Full Bathroom', 'Price per Total Bathroom',\n",
    "       'Price per Story', 'Price per Garage Space', 'Price per Living Area',\n",
    "       'Price per Lot Size Acre', 'Price per Year Built', 'suspicious_diction',\n",
    "       'phone_is_suspicious'''\n",
    "\n",
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select relevant features for anomaly detection\n",
    "features = final_df.columns\n",
    "\n",
    "print(features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Step 1: Replace infinite values with NaN\n",
    "final_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Step 2: Fill NaN values (e.g., with the median of each column)\n",
    "final_df.fillna(final_df.median(), inplace=True)\n",
    "\n",
    "X_scaled = scaler.fit_transform(final_df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit Isolation Forest model\n",
    "isolation_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "final_df['isolation_forest_flag'] = isolation_forest.fit_predict(X_scaled)\n",
    "\n",
    "# Anomalies are flagged as -1\n",
    "anomalies_if = final_df[final_df['isolation_forest_flag'] == -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit LOF model\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "final_df['lof_flag'] = lof.fit_predict(X_scaled)\n",
    "\n",
    "# LOF also flags anomalies as -1\n",
    "anomalies_lof = final_df[final_df['lof_flag'] == -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Prepare a label where 0 is normal and 1 is anomaly\n",
    "final_df['anomaly_label'] = (final_df['suspicious_diction'] == 1).astype(int)\n",
    "\n",
    "# Prepare the DMatrix for XGBoost\n",
    "X = final_df[features]\n",
    "y = final_df['anomaly_label']\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Train the XGBoost model (we can treat it as a classification problem)\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'scale_pos_weight': len(y) / y.sum()  # Adjust for class imbalance\n",
    "}\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Predict anomaly scores (using threshold 0.5 to flag anomalies)\n",
    "final_df['xgboost_flag'] = (xgb_model.predict(dtrain) > 0.5).astype(int)\n",
    "\n",
    "# Flag XGBoost anomalies\n",
    "anomalies_xgb = final_df[final_df['xgboost_flag'] == 1]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine flags: Check if all three models flagged the same listing\n",
    "final_df['flagged_by_all'] = (\n",
    "    (final_df['isolation_forest_flag'] == -1) & \n",
    "    (final_df['lof_flag'] == -1)) #& \n",
    "    # (final_df['xgboost_flag'] == 1)\n",
    "#)\n",
    "\n",
    "# Get the listings flagged by all models\n",
    "anomalies_all_models = final_df[final_df['flagged_by_all']]\n",
    "\n",
    "# Display the results\n",
    "anomalies_all_models.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_df.index, final_df['Price per Bedroom'], \n",
    "            c=final_df['isolation_forest_flag'], cmap='coolwarm', label='Anomaly Score')\n",
    "plt.title('Isolation Forest - Anomaly Detection based on Price per Bedroom')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Price per Bedroom')\n",
    "plt.colorbar(label='Flag (-1: Anomaly, 1: Normal)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOF Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_df.index, final_df['Price per Bedroom'], \n",
    "            c=final_df['lof_flag'], cmap='coolwarm', label='Anomaly Score')\n",
    "plt.title('LOF - Anomaly Detection based on Price per Bedroom')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Price per Bedroom')\n",
    "plt.colorbar(label='Flag (-1: Anomaly, 1: Normal)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# XGBoost Probability Scatter Plot\n",
    "xgboost_probs = xgb_model.predict(dtrain)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_training_set.index, xgboost_probs, c=(xgboost_probs > 0.5).astype(int), cmap='coolwarm')\n",
    "plt.title('XGBoost - Predicted Anomaly Probabilities')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Anomaly Probability')\n",
    "plt.colorbar(label='Flag (1: Anomaly, 0: Normal)')\n",
    "plt.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Count how many were flagged by each model\n",
    "isolation_flags = set(final_df.index[final_df['isolation_forest_flag'] == -1])\n",
    "lof_flags = set(final_df.index[final_df['lof_flag'] == -1])\n",
    "\n",
    "# Create a Venn diagram comparing the results of Isolation Forest and LOF\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn = venn2([isolation_flags, lof_flags], \n",
    "             set_labels=('Isolation Forest', 'LOF'))\n",
    "\n",
    "plt.title('Venn Diagram of Anomalies Flagged by Isolation Forest and LOF')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final_df.index, final_df['Price per Bedroom'], \n",
    "            c=final_df['flagged_by_all'], cmap='coolwarm')\n",
    "plt.title('Listings Flagged as Anomalies by All Models')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('List Price')\n",
    "plt.colorbar(label='Flag (1: Anomaly by all models, 0: Normal)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of listings flagged as anomalies by either Isolation Forest or LOF\n",
    "anomalies_by_either = isolation_flags.union(lof_flags)\n",
    "anomalies_by_either_list = list(anomalies_by_either)\n",
    "\n",
    "# Filter the DataFrame to get the listings marked as anomalies by either model\n",
    "anomalous_listings = final_df.loc[anomalies_by_either_list]\n",
    "\n",
    "# Print the listings marked as anomalies by either model\n",
    "print(\"Listings flagged as anomalies by either Isolation Forest or LOF:\")\n",
    "print(anomalous_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming final_df is your updated DataFrame\n",
    "# Step 1: Select only the numerical features from final_df for PCA\n",
    "numeric_features = final_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Step 2: Extract the numeric features from final_df\n",
    "X = final_df[numeric_features].values\n",
    "\n",
    "# Step 3: Standardize the numeric features before applying PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Check the shape of X_scaled to confirm preprocessing\n",
    "print(X_scaled.shape)\n",
    "\n",
    "# Step 4: Apply PCA, retaining 95% of the variance\n",
    "pca = PCA(n_components=0.95)  # Retaining 95% variance\n",
    "\n",
    "x_pca = pca.fit_transform(X_scaled)  # Perform PCA on scaled data\n",
    "\n",
    "# Step 5: Create a DataFrame for the PCA-transformed data\n",
    "pca_columns = [f'PC{i+1}' for i in range(x_pca.shape[1])]\n",
    "\n",
    "# Get the top contributing features for each principal component\n",
    "top_features_per_pc = []\n",
    "for i in range(pca.components_.shape[0]):\n",
    "    top_feature_index = np.argmax(np.abs(pca.components_[i]))\n",
    "    top_feature = numeric_features[top_feature_index]\n",
    "    top_features_per_pc.append(top_feature)\n",
    "\n",
    "# Create more descriptive principal component labels\n",
    "pca_columns_descriptive = [f'PC{i+1} ({top_features_per_pc[i]})' for i in range(len(pca_columns))]\n",
    "\n",
    "pca_df = pd.DataFrame(x_pca, columns=pca_columns)\n",
    "\n",
    "# Step 6: Visualize the explained variance for each principal component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Get the loadings (contributions) of each original feature to each principal component\n",
    "loadings = pca.components_.T  # Transpose to get original features as rows\n",
    "contributions_df = pd.DataFrame(loadings, index=numeric_features, columns=pca_columns)\n",
    "\n",
    "# Step 5: Generate a heatmap of the contributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contributions_df, cmap='coolwarm', annot=True, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Heatmap of Feature Contributions to Principal Components')\n",
    "plt.show()\n",
    "\n",
    "# Limit to a reasonable number of principal components (e.g., 10 for visualization)\n",
    "pca_df_limited = pca_df.iloc[:, :17]\n",
    "\n",
    "# Generate the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=pca_df_limited)\n",
    "\n",
    "# Fix the xticks and labels\n",
    "plt.title('Boxplot of Principal Components')\n",
    "plt.xticks(rotation=90, ticks=np.arange(len(pca_df_limited.columns)), labels=pca_columns_descriptive[:17])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Step 6: Boxplot for each principal component\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# # pca_df.boxplot()\n",
    "# sns.boxplot(data=pca_df)\n",
    "\n",
    "# plt.title('Boxplot of Principal Components')\n",
    "# plt.xticks(rotation=90, ticks=np.arange(1, len(pca_columns_descriptive) + 1), labels=pca_columns_descriptive)\n",
    "# plt.show()\n",
    "\n",
    "#Step 7: Perform DBSCAN on the PCA-transformed data\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can adjust eps and min_samples based on your data\n",
    "dbscan_labels = dbscan.fit_predict(x_pca)\n",
    "\n",
    "# Step 8: Add the DBSCAN labels to the PCA DataFrame\n",
    "pca_df['DBSCAN_label'] = dbscan_labels\n",
    "\n",
    "# Step 9: Visualize the DBSCAN results using a scatter plot of the first two principal components\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='DBSCAN_label', data=pca_df, palette='coolwarm')\n",
    "plt.title('DBSCAN Clustering on Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top contributing features for each principal component\n",
    "top_features_per_pc = []\n",
    "for i in range(pca.components_.shape[0]):\n",
    "    top_feature_index = np.argmax(np.abs(pca.components_[i]))\n",
    "    top_feature = numeric_features[top_feature_index]\n",
    "    top_features_per_pc.append(top_feature)\n",
    "\n",
    "# Create more descriptive principal component labels\n",
    "pca_columns_descriptive = [f'PC{i+1} ({top_features_per_pc[i]})' for i in range(len(pca_columns))]\n",
    "\n",
    "# Update the heatmap with descriptive labels\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contributions_df, cmap='coolwarm', annot=True, fmt='.2f', linewidths=0.5)\n",
    "plt.title('Feature Contribution to Principal Components', fontsize=16)\n",
    "plt.xlabel('Principal Components (Top Feature in Parentheses)', fontsize=12)\n",
    "plt.ylabel('Feature Names', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(len(pca_columns_descriptive)) + 0.5, labels=pca_columns_descriptive, rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n",
    "\n",
    "# Use descriptive labels for the principal components in the explained variance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Variance Explained by Principal Components', fontsize=16)\n",
    "plt.xlabel('Principal Components (Top Feature)', fontsize=12)\n",
    "plt.ylabel('Variance Ratio (Explained Variance %)', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(1, len(pca_columns_descriptive) + 1), labels=pca_columns_descriptive, rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Ensure layout doesn't overlap\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_financial_damage(anomalies_df, precision=0.8, scam_success_rate=0.5):\n",
    "    \"\"\"\n",
    "    Estimate financial damages caused by scams based on anomalies.\n",
    "\n",
    "    Parameters:\n",
    "    anomalies_df (DataFrame): The dataframe of detected anomalies.\n",
    "    precision (float): The precision rate of actual scams within anomalies (default=0.8).\n",
    "    scam_success_rate (float): The success rate of scams succeeding (default=0.5).\n",
    "\n",
    "    Returns:\n",
    "    float: Estimated financial damages.\n",
    "    \"\"\"\n",
    "    # Calculate the estimated number of actual scams\n",
    "    estimated_scams = len(anomalies_df) * precision * scam_success_rate\n",
    "    \n",
    "    # Calculate the mean price of the anomalies\n",
    "    mean_price = anomalies_df['Price per Story'].mean()\n",
    "    \n",
    "    # Estimate total financial damage\n",
    "    estimated_damage = estimated_scams * mean_price\n",
    "    \n",
    "    return estimated_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_financial_damage(anomalous_listings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anomalous_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalous_listings.sort_values(by = \"Distance Suspiciousness\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_for_detection = final_df.columns\n",
    "features_for_detection\n",
    "\n",
    "print(final_df['Under_30_Days_Flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Prepare the data for anomaly detection (select relevant columns)\n",
    "# Assuming df_price_metrics is the DataFrame with price metrics\n",
    "features_for_detection = final_df.columns\n",
    "\n",
    "# Fill NaN values with the median of each column (or another appropriate strategy)\n",
    "features_for_detection = features_for_detection.fillna(features_for_detection.median())\n",
    "\n",
    "# Fit the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # Adjust contamination rate\n",
    "iso_forest.fit(features_for_detection)\n",
    "\n",
    "# Get anomaly scores (-1 means anomaly)\n",
    "anomaly_labels = iso_forest.predict(features_for_detection)\n",
    "df_price_metrics['Anomaly'] = anomaly_labels\n",
    "\n",
    "# Filter out anomalies (scam candidates)\n",
    "anomalous_listings = df_price_metrics[df_price_metrics['Anomaly'] == -1]\n",
    "\n",
    "# Display the anomalies\n",
    "print(f\"Number of anomalies detected: {len(anomalous_listings)}\")\n",
    "anomalous_listings.head()\n",
    "'''\n",
    "\n",
    "# Select only numerical columns for the anomaly detection\n",
    "numerical_features = final_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Fill NaN values with the median of each column (or another appropriate strategy)\n",
    "numerical_features = numerical_features.fillna(numerical_features.median())\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical data and transform it\n",
    "scaled_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame and assign column names\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=numerical_features.columns)\n",
    "\n",
    "# Replace the original numerical features in final_df with the scaled ones\n",
    "final_df[numerical_features.columns] = scaled_df\n",
    "\n",
    "# Fit the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # Adjust contamination rate\n",
    "iso_forest.fit(numerical_features)\n",
    "\n",
    "# Get anomaly scores (-1 means anomaly)\n",
    "anomaly_labels = iso_forest.predict(numerical_features)\n",
    "final_df['Anomaly'] = anomaly_labels\n",
    "\n",
    "# Filter out anomalies (scam candidates)\n",
    "\n",
    "# Display the anomalies\n",
    "print(f\"Number of anomalies detected: {len(anomalous_listings)}\")\n",
    "anomalous_listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Re-apply Min-Max Scaling on the numerical columns (excluding irrelevant ones)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler and transform the clean numerical data\n",
    "scaled_data_min_max = min_max_scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame for easier manipulation\n",
    "scaled_df_min_max = pd.DataFrame(scaled_data_min_max, columns=numerical_columns)\n",
    "\n",
    "# Fit the Isolation Forest model on the Min-Max scaled data\n",
    "iso_forest_min_max = IsolationForest(contamination=0.05, random_state=42)\n",
    "iso_forest_min_max.fit(scaled_df_min_max)\n",
    "\n",
    "# Get anomaly labels (-1 means anomaly)\n",
    "anomaly_labels_min_max = iso_forest_min_max.predict(scaled_df_min_max)\n",
    "\n",
    "# Adding the anomaly labels to the original dataframe\n",
    "final_df['Anomaly'] = anomaly_labels_min_max\n",
    "\n",
    "# Filter out anomalies (scam candidates)\n",
    "anomalous_listings_min_max = final_df[final_df['Anomaly'] == -1]\n",
    "\n",
    "# Displaying the number of anomalies and first few anomalous rows\n",
    "len(anomalous_listings_min_max), anomalous_listings_min_max.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create a 3D plot\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot normal listings\n",
    "ax.scatter(final_df[final_df['Anomaly'] == 1]['Price per Story'],\n",
    "           final_df[final_df['Anomaly'] == 1]['Price per Bedroom'],\n",
    "           final_df[final_df['Anomaly'] == 1]['Distance Suspiciousness'],\n",
    "           c='blue', label='Normal Listings', alpha=0.6)\n",
    "\n",
    "# Plot anomalous listings\n",
    "ax.scatter(final_df[final_df['Anomaly'] == -1]['Price per Story'],\n",
    "           final_df[final_df['Anomaly'] == -1]['Price per Bedroom'],\n",
    "           final_df[final_df['Anomaly'] == -1]['Distance Suspiciousness'],\n",
    "           c='red', label='Anomalies (Scams)', alpha=0.8)\n",
    "\n",
    "# Labeling\n",
    "ax.set_title('Anomaly Detection - 3D Plot')\n",
    "ax.set_xlabel('Price per Story')\n",
    "ax.set_ylabel('Price per Bedroom')\n",
    "ax.set_zlabel('Distance Suspiciousness')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'final_df' is your DataFrame with all the features\n",
    "# Select only numerical columns for scaling\n",
    "# Select only numerical columns for the anomaly detection\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numerical data and transform it\n",
    "scaled_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame and assign column names\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=numerical_features.columns)\n",
    "\n",
    "# Replace the original numerical features in final_df with the scaled ones\n",
    "final_df[numerical_features.columns] = scaled_df\n",
    "\n",
    "# Now, final_df is standardized and ready for training with Isolation Forest\n",
    "\n",
    "# Initialize the Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model on the standardized data\n",
    "iso_forest.fit(final_df.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Predict anomalies (-1 means anomaly, 1 means normal)\n",
    "anomaly_labels = iso_forest.predict(final_df.select_dtypes(include=[np.number]))\n",
    "\n",
    "# Add the anomaly labels to the DataFrame\n",
    "final_df['Anomaly'] = anomaly_labels\n",
    "\n",
    "# Filter out the anomalies (scam candidates)\n",
    "anomalous_listings = final_df[final_df['Anomaly'] == -1]\n",
    "\n",
    "# Display the anomalies\n",
    "print(f\"Number of anomalies detected: {len(anomalous_listings)}\")\n",
    "print(anomalous_listings.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
